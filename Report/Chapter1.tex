\chapter{Introduction}
Neural networks have seen a surge in popularity ever since a neural network, coined AlexNet, decisively won the ImageNet Large Scale Visual Recognition Challenge in 2012, achieving 10.8\% higher accuracy then the next best solution \cite{Krizhevsky}. It was the only neural network entry in the entire competition; and the victory stunned much of the academic world.

This moment has been generally regarded as the spark that ignited the massive surge in academic interest toward neural networks and statistical machine learning. In the 7 years since, research regarding neural networks has yet to slow down as real-world applications of neural networks continue to be expand. To name a few examples, neural networks are currently in use for facial recognition at Facebook \cite{deepface}, translation for Microsoft \cite{translation}, spam filters for Google's Gmail \cite{gmail} and endless more.

In order for these neural networks to have such stellar accuracy on tasks such as image classification, they must first learn from labeled data in a process known as training. The neural network training process has an incredibly high level of inherent parallelism, and thus GPUs have emerged as the device of choice to train neural networks. GPU-based training takes advantage of data-level parallelism to train networks by assigning separate images in a training batch to different cores, which then all perform the same computations on different data. This coarsely-grained approach to training works well for large batch sizes. However, since these GPU models use data parallelism for speedup, GPU training models degrades for small batch sizes and are even slower than CPU models when it comes to online learning. Online learning is when a single labeled data sample is fed to the network at a time, or in other words, when the batch size is equal to 1 \cite{batch-size}. 

Today's solutions for training neural networks with small batch sizes do not take advantage of the fine-grain parallelism available in neural networks. This thesis presents a hardware accelerator that uses fine-grain parallelism at the neuron level to achieve high training performance. The accelerator results in much faster performance compared to the current solutions used by academia and industry when training neural networks with small batch sizes.

This thesis proposes a novel hardware architecture for the training of neural networks. While the focus of the thesis is the architectural design of the hardware accelerator, a basic understanding of neural networks is helpful. As such, Chapter \ref{background} reviews the basics of neural networks and surveys related work on designing hardware to optimize for neural network computation. Chapter \ref{ch-sw-model} describes the software model that was implemented to verify and further understand the algorithms to be used for the hardware model. Chapter \ref{hw-model} covers the hardware model and implementation of the accelerator. Next, Chapter \ref{hw-model-testing} documents the testing methods used to functionally verify the hardware model. Chapter \ref{results} presents the results of the thesis and Chapter \ref{analysis} provides analysis of these results. Chapter \ref{discussion} discusses the project as a whole and what future work could be done to improve the project. Finally, Chapter \ref{conclusion} presents the conclusion of the thesis.

