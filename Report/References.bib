%==================================================================================================
% LUKE PHD THESIS BIBTEX FILE
% ---------------------------
% Sorted chronologically
%==================================================================================================

@article{HeZR015,
	author    = {Kaiming He and
	Xiangyu Zhang and
	Shaoqing Ren and
	Jian Sun},
	title     = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on
	ImageNet Classification},
	journal   = {CoRR},
	volume    = {abs/1502.01852},
	year      = {2015},
	url       = {http://arxiv.org/abs/1502.01852},
	archivePrefix = {arXiv},
	eprint    = {1502.01852},
	timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/HeZR015},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@MASTERSTHESIS {xinbochen2016,
	author = {Xinbo Chen},
	title  = {ENERGY EFFICIENCY ANALYSIS AND OPTIMIZATION OF CONVOLUTIONAL NEURAL NETWORKS FOR IMAGE RECOGNITION},
	school = {Texas State University},
	year   = {2016},
	month  = {may}
}

@inproceedings{Courbariaux2014TrainingDN,
	title={Training deep neural networks with low precision multiplications},
	author={Matthieu Courbariaux and Yoshua Bengio and Jean-Pierre David},
	year={2014}
}

@misc{gmail, title={The mail you want, not the spam you don't}, 
	url={https://gmail.googleblog.com/2015/07/the-mail-you-want-not-spam-you-dont.html},
	note={\url{https://gmail.googleblog.com/2015/07/the-mail-you-want-not-spam-you-dont.html}},
	journal={Official Gmail Blog}, 
	year={2015}, 
	month={Jul}
}

@inproceedings{TABLA,
	author = {Mahajan, Divya and Park, Jongse and Amaro, Emmanuel and Sharma, Hardik and Yazdanbakhsh, Amir and Kyung Kim, Joon and Esmaeilzadeh, Hadi},
	year = {2016},
	month = {03},
	pages = {14-26},
	title = {TABLA: A unified template-based framework for accelerating statistical machine learning},
	doi = {10.1109/HPCA.2016.7446050}
}

@inproceedings{eyeriss,
	author = {Chen, Yu-Hsin and Emer, Joel and Sze, Vivienne},
	title = {Eyeriss: A Spatial Architecture for Energy-efficient Dataflow for Convolutional Neural Networks},
	booktitle = {Proceedings of the 43rd International Symposium on Computer Architecture},
	series = {ISCA '16},
	year = {2016},
	isbn = {978-1-4673-8947-1},
	location = {Seoul, Republic of Korea},
	pages = {367--379},
	numpages = {13},
	url = {https://doi.org/10.1109/ISCA.2016.40},
	doi = {10.1109/ISCA.2016.40},
	acmid = {3001177},
	publisher = {IEEE Press},
	address = {Piscataway, NJ, USA},
} 

@inproceedings{TPU,
	author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
	title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
	booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
	series = {ISCA '17},
	year = {2017},
	isbn = {978-1-4503-4892-8},
	location = {Toronto, ON, Canada},
	pages = {1--12},
	numpages = {12},
	url = {http://doi.acm.org/10.1145/3079856.3080246},
	doi = {10.1145/3079856.3080246},
	acmid = {3080246},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {CNN, DNN, GPU, LSTM, MLP, RNN, TPU, TensorFlow, accelerator, deep learning, domain-specific architecture, neural network},
} 

@article{FCNN,
	title={F-CNN: An FPGA-based framework for training Convolutional Neural Networks},
	author={Wenlai Zhao and Haohuan Fu and Wayne Luk and Teng Yu and Shaojun Wang and Bo Feng and Yuchun Ma and Guangwen Yang},
	journal={2016 IEEE 27th International Conference on Application-specific Systems, Architectures and Processors (ASAP)},
	year={2016},
	pages={107-114}
}

@misc{petalinux,
	title  = {Zynq 2016.4 Release},
	author = {XilinxWiki},
	url		= {https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/18842326/Zynq+2016.4+Release},
	note    = {\url{https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/18842326/Zynq+2016.4+Release}}
}
@article{translation,
	author    = {Yingce Xia and
	Di He and
	Tao Qin and
	Liwei Wang and
	Nenghai Yu and
	Tie{-}Yan Liu and
	Wei{-}Ying Ma},
	title     = {Dual Learning for Machine Translation},
	journal   = {CoRR},
	volume    = {abs/1611.00179},
	year      = {2016},
	url       = {http://arxiv.org/abs/1611.00179},
	archivePrefix = {arXiv},
	eprint    = {1611.00179},
	timestamp = {Mon, 13 Aug 2018 16:46:54 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/XiaHQWYLM16},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{sm-derivative, title={The Softmax function and its derivative}, note={\url{https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/}}, journal={Eli Benderskys website}}

@misc{q-format,
	title  = {ARM Developer Suite AXD and armsd Debuggers Guide},
	author = {ARM Info center},
	note    = {\url{http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0066d/CHDFAAEI.html}},
	year   = {2001}
}


@inproceedings{deepface,
	author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
	year = {2014},
	month = {09},
	pages = {},
	title = {DeepFace: Closing the Gap to Human-Level Performance in Face Verification},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	doi = {10.1109/CVPR.2014.220}
}

@online{grad-check-stanford,
	author = {Andrej Karpathy},
	title  = {CS231n Convolutional Neural Networks for Visual Recognition},
	url    = {http://cs231n.github.io/neural-networks-3},
	note   = {\url{http://cs231n.github.io/neural-networks-3}}
}

@article{Lu2019DyingRA,
	title={Dying ReLU and Initialization: Theory and Numerical Examples},
	author={Lu Lu and Yeonjong Shin and Yanhui Su and George Em Karniadakis},
	journal={CoRR},
	year={2019},
	volume={abs/1903.06733}
}
@InProceedings{pmlr-v15-glorot11a,
	title = 	 {Deep Sparse Rectifier Neural Networks},
	author = 	 {Xavier Glorot and Antoine Bordes and Yoshua Bengio},
	booktitle = 	 {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
	pages = 	 {315--323},
	year = 	 {2011},
	editor = 	 {Geoffrey Gordon and David Dunson and Miroslav Dud√≠k},
	volume = 	 {15},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Fort Lauderdale, FL, USA},
	month = 	 {11--13 Apr},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf},
	url = 	 {http://proceedings.mlr.press/v15/glorot11a.html},
	abstract = 	 {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training. [pdf]}
}