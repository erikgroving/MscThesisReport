%==================================================================================================
% LUKE PHD THESIS BIBTEX FILE
% ---------------------------
% Sorted chronologically
%==================================================================================================

@article{HeZR015,
	author    = {Kaiming He and
	Xiangyu Zhang and
	Shaoqing Ren and
	Jian Sun},
	title     = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on
	ImageNet Classification},
	journal   = {CoRR},
	volume    = {abs/1502.01852},
	year      = {2015},
	url       = {http://arxiv.org/abs/1502.01852},
	archivePrefix = {arXiv},
	eprint    = {1502.01852},
	timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/HeZR015},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{gmail, title={The mail you want, not the spam you don't}, url={https://gmail.googleblog.com/2015/07/the-mail-you-want-not-spam-you-dont.html}, journal={Official Gmail Blog}, year={2015}, month={Jul}}

@article{translation,
	author    = {Yingce Xia and
	Di He and
	Tao Qin and
	Liwei Wang and
	Nenghai Yu and
	Tie{-}Yan Liu and
	Wei{-}Ying Ma},
	title     = {Dual Learning for Machine Translation},
	journal   = {CoRR},
	volume    = {abs/1611.00179},
	year      = {2016},
	url       = {http://arxiv.org/abs/1611.00179},
	archivePrefix = {arXiv},
	eprint    = {1611.00179},
	timestamp = {Mon, 13 Aug 2018 16:46:54 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/XiaHQWYLM16},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{sm-derivative, title={The Softmax function and its derivative}, url={https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/}, journal={Eli Benderskys website}}

@inproceedings{deepface,
	author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
	year = {2014},
	month = {09},
	pages = {},
	title = {DeepFace: Closing the Gap to Human-Level Performance in Face Verification},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	doi = {10.1109/CVPR.2014.220}
}

@ONLINE {grad-check-stanford,
	author = {Andrej Karpathy},
	title  = {CS231n Convolutional Neural Networks for Visual Recognition},
	url    = {http://cs231n.github.io/neural-networks-3}
}

@article{Lu2019DyingRA,
	title={Dying ReLU and Initialization: Theory and Numerical Examples},
	author={Lu Lu and Yeonjong Shin and Yanhui Su and George Em Karniadakis},
	journal={CoRR},
	year={2019},
	volume={abs/1903.06733}
}
@InProceedings{pmlr-v15-glorot11a,
	title = 	 {Deep Sparse Rectifier Neural Networks},
	author = 	 {Xavier Glorot and Antoine Bordes and Yoshua Bengio},
	booktitle = 	 {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
	pages = 	 {315--323},
	year = 	 {2011},
	editor = 	 {Geoffrey Gordon and David Dunson and Miroslav Dud√≠k},
	volume = 	 {15},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Fort Lauderdale, FL, USA},
	month = 	 {11--13 Apr},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf},
	url = 	 {http://proceedings.mlr.press/v15/glorot11a.html},
	abstract = 	 {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training. [pdf]}
}