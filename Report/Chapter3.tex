\chapter{Software Model}
\label{ch-sw-model}
\todo[inline]{figure for code snippets or nah?}
\section{Overview}
This section documents the general-purpose neural network framework that was written in C++ for this thesis. There is an example program that trains on the MNIST dataset and documents epoch-by-epoch training statistics. MNIST is a dataset of handwritten digits, containing 60,000 training images and 10,000 test images. The source code for the software model can be found in the appendix as well as online on GitHub.\footnote{\url{
https://github.com/erikgroving/NeuralNetworkHardwareAccelerator/tree/master/SWModel}.}


\section{Motivation}
The software neural network framework was written so that the FPGA hardware model could be benchmarked against a CPU-based model that performs neural network inference and backward passes using the same method as the hardware model. This benchmark could be used to evaluate the performance of the hardware model. In addition, it could be benchmarked against professional open-source deep-learning frameworks that make use of advanced algebraic methods to perform computation such as matrix multiplication that inherently offer more efficiency. Furthermore, by developing a software model, the algorithmic integrity of the proposed network was able to verified and tested in an expedient manner by using a well-known testing framework, Google Test. Finally, if high floating-point precision were needed for training a network, then the software model could be used to learn the weights and parameters, and then subsequently be loaded into the weight BRAM of the FPGA hardware model.

\section{Design}
\subsection{Layers}
The software model was designed to be flexible such that any neural network architecture may be constructed so long as the layer types were implemented. The model currently supports 2D convolutional, fully connected, and pooling layers. 
\par 
All layers are derived from a base class, \texttt{Layer}. Certain methods such as \texttt{forward()} and \texttt{backward()} must be implemented by all derived classes. There is then a \texttt{Net} class that contains a \texttt{vector} of \texttt{Layer} objects. This allows for a flexible design, as one only need add layers to the \texttt{Net} object. Furthermore, the model can easily be extended to other layer types so long as the layer type derives from \texttt{Layer}. 
\par
The non-linear activation function used in the model is ReLU because the derivative is trivial to compute. Compared to the sigmoid function, ReLU is much more computationally feasible for an FPGA hardware implementation, and thereofre, ReLU was used in the software model so that both models would use the same activation function.
\subsection{Training}
\paragraph{The Softmax Function and Computing Loss Gradients}
The network uses an implicit Softmax function for the last layer since this converts the logits in the last layer to numbers that can be interpreted as probabilities, ideal for image classification. 
\par
The loss gradients for the neurons in the last layer are computed using multi-class cross entropy loss. Therefore, only one probability will account for loss, however, since each probability is an output from the softmax function which takes in all neuron outputs as input, all neurons in the last layer will have a loss gradient.
\par 
The derivative of this loss function is needed to perform backpropagation. We define $\mathcal{L}_i$ as the loss for neuron $i$ in the last layer and $z_i$ as the output of neuron $i$. We also introduce $y_i$, which is 1 if $x$ is an instance of class $i$ and 0 otherwise. We can then compute the loss gradient for neuron $i$ in the last layer quite simply as follows: 
\[    
\frac{\delta \mathcal{L}_i}{\delta z_i} = z_i - y_i
\]

\paragraph{Batch Size}
The software model supports batch training and thus a batch size is to be specified when creating an instance of a new network.
\paragraph{Learning Rate and Momentum}
The software model learns using stochastic gradient descent. As such, the network is configured with a learning rate and momentum. The learning rate may be manually readjusted during training epochs.



\section{Source Code Structure}
The software model contains a Makefile and three folders: \textit{data}, \textit{src} and \textit{test}. The \textit{data} folder contains the MNIST binary data files, and is loaded by the example program that trains on the MNIST dataset. The \textit{src} folder contains the source code of the neural network framework. The \textit{test} folder contains test made using the Google Test C++ testing framework. The Makefile is used to build the source as well as tests. This section will detail the source files in the \textit{src} folder that are core to the software model framework. The files \textit{main.cpp} and \textit{parse\_data\{.cpp, .h\}} will be described in section \ref{sw-usage} that focuses on usage.

\paragraph{net\{.cpp, .h\}}
These files contain the definition of the \texttt{Net} class, the highest-level class of the network. After initializing a \texttt{Net} object, layers can be added to the neural network by calling the \texttt{addLayer()} method which will add a \texttt{Layer} object to a \texttt{vector}. The \texttt{Net} class also stores intermediate activations from the current inference, which are required when performing backward pass to calculate loss gradients. The key parameters to the \texttt{Net} object are set in its constructor, and are defined in table \ref{nettable}. 
\begin{table}
	\centering
	\begin{tabularx}{\textwidth}{|l|l|X|}
		\hline
		\textbf{Name} 			& \textbf{Type} 		& \textbf{Description} \\\hline
		\texttt{in}  			& \texttt{uint32\_t}	& Size of the input to the neural network.\\\hline
		\texttt{out}			& \texttt{uint32\_t}	& Size of the output of the neural network. \\\hline 
		\texttt{bs}				& \texttt{uint32\_t}	& Size of the batch size to be used when training the net.\\\hline 
		\texttt{lr}				& \texttt{double}	& The learning rate to be used during training of the network. Can be set and read using the functions \texttt{setLearningRate()} and \texttt{getLearningRate()}. \\\hline 
		\texttt{momentum}		& \texttt{double}	& The momentum to be used when performing updates to the weights and biases of the network.
		\\\hline
	\end{tabularx}
	\caption{Description of parameters for the constructor \texttt{Net} class.}
	\label{nettable}
\end{table}
\par 
The \texttt{Net} class has a method \texttt{inference()} that computes the forward pass for a batch of inputs, thus the argument is a 2-d \texttt{vector}, with each outer index corresponding to an input. The \texttt{()} operator has also been overloaded to call \texttt{inference()}. This is all that is needed to compute a forward pass.
\par 
To compute the backward pass, \texttt{computeLossAndGradients()} should be called first. This method takes in the label data as a \texttt{vector} for the inputs as an argument and computes the loss gradients for the outer layer of the network. Next, a call to \texttt{backpropLoss()} should be made; this method propagates the outer layer loss gradients back through the neural network. After the loss has been back-propagated, weights of each \texttt{Neuron} in the network should be updated by calling \texttt{update()}. Previously cached forward pass activation data should then be cleared with a call to \texttt{clearSavedData()}.
\paragraph{layer.h}
This file contains the \texttt{Layer} class, which serves as the base class for all the different types of layer classes in the framework. It contains virtual methods \texttt{forward()} and \texttt{backward()}, representing the forward and backward pass functionality that must be implemented. All layer classes must also contain a \texttt{getType()} method to identify the layer type, as well as methods for \texttt{updateWeights()}, \texttt{clearData()}, and \texttt{getOutput()}.

\paragraph{convolutional\{.cpp, .h\}}
These files contain the definition of the \texttt{ConvLayer} class, which implements a 2D-convolutional layer, and derives from the \texttt{Layer} class. A unique method to the \texttt{ConvLayer} class is the \texttt{getWindowPixels()} method, which returns the pixels inside the filter window, and is used when computing both the forward and backward passes.  The class' constructor and key parameters are described in table \ref{convtable}.
\begin{table}
\centering
\begin{tabularx}{\textwidth}{|l|l|X|}
	\hline
	\textbf{Name} 			& \textbf{Type} 		& \textbf{Description} \\\hline
	\texttt{dim}  			& \texttt{uint32\_t}	& Dimensions of the input. The dimension is assumed square, meaning that rows = \texttt{dim} and columns = \texttt{dim}.\\\hline
	\texttt{filt\_size} 	& \texttt{uint32\_t}	& Dimension of the filter used for the convolution, dimension also assumed square. \\\hline 
		\texttt{stride} 	& \texttt{uint32\_t}	& Size of the stride \\\hline 
	\texttt{padding} 		& \texttt{uint32\_t}	& Padding used for convolution. \\\hline 
	\texttt{in\_channels} 	& \texttt{uint32\_t}	& Amount of channels in the input. \\\hline 
	\texttt{out\_channels} 	& \texttt{uint32\_t}	& Amount of channels in the output. \\\hline
\end{tabularx}
\caption{Description of parameters for the \texttt{ConvLayer} class.}
\label{convtable}
\end{table}


\paragraph{fullyconnected\{.cpp,.h\}}
These files define the \texttt{FullyConnected} class. The class only has two defining parameters in its constructor: \texttt{in} and \texttt{out}, which are of type \texttt{uint32\_t} and specify the input and output size to the layer, respectively. It derives from the base \texttt{Layer} class, so methods such as \texttt{forward()} and \texttt{backward()} are also implemented.

\paragraph{pooling\{.cpp,.h\}}
These files define the \texttt{PoolingLayer} class. The class derives from \texttt{Layer} and performs a 2D 2$\times$2 max pooling operation. There are three main parameters for the class: \texttt{dim\_i}, \texttt{dim\_o}, and \texttt{channels}. The parameters \texttt{dim\_i} and \texttt{dim\_o} specify the dimension of the input and output feature vectors. Since the layer currently only performs 2$\times$2 max pooling, \texttt{dim\_o} will always be half of \texttt{dim\_i}, though if different types of pooling filters were to be supported, then \texttt{dim\_o} would be necessary. The \texttt{channels} parameter is used to specify the number of channels of size \texttt{dim\_i} $\times$ \texttt{dim\_i} present in the input.

\paragraph{neuron\{.cpp, .h\}}
These files define the \texttt{Neuron} class. The \texttt{Neuron} class is the computational building block of the fully connected and convolutional layers. The fan-in of the neuron is specified in the constructor. Weights should be initialized using the \texttt{initWeights()} method, which implements He initialization \cite{HeZR015}. He initialization randomly initializes weights using a normal distribution with a mean of 0 and a variance of $\frac{2}{\text{fan\_in}}$. 
\par 
The class implements all necessary computational elements for a neuron in a neural network. During a forward pass, a neuron's net and activation are computed with \texttt{computeNet()} and \texttt{computeActivation()} respectively. When computing the backward pass, the gradients for the neuron's weights are computed using \texttt{calculateGradient()}. Weights can be subsequently updated using the \texttt{updateWeights()} function. Finally, all gradient data can be cleared using \texttt{clearBackwardData()}.

\section{Usage}\label{sw-usage}
This section will show how the software model may be used for image classification. In the following example, the software model will be trained to classify handwritten digits from the MNIST database. Each image is a handwritten digit of size 28$\times$28. The relevant files specific to this example are \textit{main.cpp} and \textit{parse\_data.cpp}.
\paragraph{Load the Training and Testing Data}
The first step to any neural network problem is to load the training and testing dataset.
The MNIST dataset is provided as binary files and helper functions to load the data have been provided in \textit{parse\_data.cpp}. Training and testing data can be loaded as shown below.
\begin{lstlisting}[language=c++]
std::vector< std::vector<double> > trainX;
std::vector<int> trainY;
std::vector< std::vector<double> > testX;
std::vector<int> testY;
trainX = readImages("data/train-images.idx3-ubyte");
trainY = readLabels("data/train-labels.idx1-ubyte");
testX = readImages("data/t10k-images.idx3-ubyte");
testY = readLabels("data/t10k-labels.idx1-ubyte");
\end{lstlisting}
\paragraph{Create a \texttt{Net} Instance}
The next step is to create a \texttt{Net} object with the relevant hyperparameters to be used for the neural network. The below code accomplishes this.
\begin{lstlisting}[language=c++]
int 	input_size  = 28*28;
int     output_size = 10;
int     batch_size  = 200;
double  momentum    = 0.9;
double  lr          = 0.01; 
Net net(input_size, output_size, batch_size, lr, momentum);
\end{lstlisting}

\paragraph{Create Layer Objects and Add them to the \texttt{Net} Object} 
After the \texttt{Net} object has been created, layers need to be added to the network. Two configuration options are present in \textit{main.cpp}; one implements a 7-layer convolutional neural network, and the other implements a 4-layer fully connected neural network. The below code snippet shows how the 7-layer convolutional neural network is implemented. The software model was designed with simplicity in mind, so the below code is relatively straightforward to follow.
\begin{lstlisting}[language=c++]
Layer* conv1 = new ConvLayer(28, 3, 1, 1, 1, 8);
Layer* pool1 = new PoolingLayer(28, 14, 8);
Layer* conv2 = new ConvLayer(14, 3, 1, 1, 8, 16);
Layer* pool2 = new PoolingLayer(14, 7, 16);
Layer* fc1 = new FullyConnected(16*7*7, 64);
Layer* fc2 = new FullyConnected(64, 10);

net.addLayer(conv1);
net.addLayer(pool1);
net.addLayer(conv2);
net.addLayer(pool2);
net.addLayer(fc1);
net.addLayer(fc2);
\end{lstlisting}

\paragraph{Train the Net}
In \textit{main.cpp}, a function \texttt{trainNet()} has been implemented, which trains the net using batch training. The actual training for a given batch only requires 5 lines of code, and is shown below.
\begin{lstlisting}[language=c++]
net(in_batch);
net.computeLossAndGradients(out_batch);
net.backpropLoss();
net.update();
net.clearSavedData();
\end{lstlisting}
\paragraph{Build and Run the Model}
Compile the code by running \texttt{make} in the \textit{SWModel} directory. The model will then train for the amount of epochs specified in the call to the \texttt{trainNet()} function in \texttt{main()}. Since the model is initialized with random weights, the final result of training is non-deterministic. Output similar to the output shown in figure \ref{sw-model-output} can be expected. In this case, the fully connected model was used, and train to a maximum accuracy of 97.62\%. it is also worth noting the expected differences in loss and accuracy between the training and test datasets. This discrepancy is expected as the network never learns from the test dataset.
The difference between test and training dataset accuracy is normally used to quantify how well the network is able to generalize from the training dataset.
\begin{figure}
\begin{lstlisting}
Running software model...
Starting Accuracy
Total correct: 1022 / 10000
Accuracy: 0.1022

Epoch: 0
--- Training Stats ---
Total correct: 54914 / 60000
Accuracy: 0.915233
Loss: 0.290908
--- Test Stats ---
Total correct: 9183 / 10000
Accuracy: 0.9183
Loss: 0.280574

Epoch: 1
--- Training Stats ---
Total correct: 56213 / 60000
Accuracy: 0.936883
Loss: 0.218062
--- Test Stats ---
Total correct: 9390 / 10000
Accuracy: 0.939
Loss: 0.214584

...

Epoch: 36
--- Training Stats ---
Total correct: 59168 / 60000
Accuracy: 0.986133
Loss: 0.0516957
--- Test Stats ---
Total correct: 9762 / 10000
Accuracy: 0.9762
Loss: 0.0845137
\end{lstlisting}
\caption{An expected output from using the software model on the provided MNIST dataset. Epochs 2-35 omitted for brevity. In this training run, the network reached a maximum test set accuracy of 97.62\%.}
\label{sw-model-output}
\end{figure}

\section{Testing}
To ensure the correctness of the software model, several test suites were created during development. Source code for the test suites can be found in the \textit{test} folder as well as in the appendix.
\todo[inline]{source code in appendix}
\subsection{Test Suites}
Four test suites were created during the development of the software model. The test cases were written to test features as they were developed. As such, the tests include neuron functionality, forward pass for fully connected and convolutional layers, and finally a gradient checking test suite to verify the backward pass. This section elaborates on the test suites that were used during development.

\paragraph{Neuron Testing}
The neuron test suite, found in \textit{neuron\_test.cpp}, contains one primary test case that sets the weights of a neuron, computes the activation, and verifies that the activation is correct.

\paragraph{Fully Connnected Forward Pass}
The test case for a fully connected layer's forward pass is located in \textit{fullyconnected\_test.cpp}. The test case creates a \texttt{FullyConnected} layer that has 3 inputs and 4 outputs. The weights are then set and an input is sent forward through the layer. Each of the 4 outputs are then verified to be correct.

\paragraph{Convolutional Forward Pass}
There is a test case to verify the convolutional forward pass located in \textit{conv\_test.cpp}. The test creates a convolutional layer that takes a 2$\times$2 feature vector with 2 channels, uses a 3$\times$3 filter for convolution, uses a stride and padding of 1, and produces 2 output channels. Weights and inputs were the arbitrarily assigned and the forward pass was computed and verified against the output that had been previously calculated manually.

\paragraph{Gradient Checking}
It would be very tedious and error-prone to debug the backward pass of a neural network using manual calculations, thus the general standard method of testing the gradients computed during a backward pass is to use gradient checking. Note that during the backward pass, all the loss gradients for every single weight and bias are calculated. For every weight (and bias), the partial derivative $\frac{\delta \mathcal{L}}{\delta w_i}$ is computed. Gradient checking verifies that the mathematically computed analytic derivative aligns with a numerically estimated derivative \cite{grad-check-stanford}. The numerical gradient can be computed as follows:
\begin{align*}
	\frac{\delta \mathcal{L}(w_i)}{\delta w_i} = \frac{\mathcal{L}(w_i + \epsilon) - \mathcal{L}(w_i - \epsilon)}{2\epsilon}
\end{align*}
The partial derivative of the loss with respect to a certain weight $w_i$ can thus be estimated by calculating the loss after incrementing $w_i$ by a small $\epsilon$, calculating the loss after decrementing $w_i$ by $\epsilon$, and then dividing the difference by $2\epsilon$. As long as $\epsilon$ is rather small, the derivatives should be near exact. In these test cases, $\epsilon = 10^{-4}$. Once we have the analytic and numerical gradient, we can compute the relative error as shown below:
\begin{align*}
	\text{Relative gradient error} = \frac{\lvert\mathcal{L}'(w_i)_a - \mathcal{L}'(w_i)_n\rvert}
		{\max \;\left(\lvert\mathcal{L}'(w_i)_a\rvert,\; \lvert\mathcal{L}'(w_i)_n\rvert\right)}
\end{align*}
If the relative error is below a certain threshhold, then it is safe to assume the gradient has been calculated correctly. In this test suite, the relative error threshhold must be lower than $10^{-7}$.
\par 
The two test cases in \textit{gradient\_check\_test.cpp} perform gradient checks for a fully connected network and for a convolutional neural network. The fully connected network gradient check test creates a neural network with an architecture shown in figure \ref{fctest}.
\begin{figure}
	\begin{lstlisting}[language=C++]
int 	input_size  = 100;
int 	output_size = 2;
int 	batch_size  = 1;
double 	momentum    = 0.9;
double 	lr          = 0.001; 
Net net(input_size, output_size, batch_size, lr, momentum);


Layer* fc1 = new FullyConnected(input_size, 98);
Layer* fc2 = new FullyConnected(98, 64);
Layer* fc3 = new FullyConnected(64, output_size);

net.addLayer(fc1);
net.addLayer(fc2);
net.addLayer(fc3);		
	\end{lstlisting}
	\caption{Layer created for the fully connected gradient check test.}
	\label{fctest}
\end{figure}
\par
The test then creates 10 random inputs, each having a random label. Each input sample is fed forward through the network and analytic gradients are computed for each weight. The numerical gradient is then subsequently computed for a random weight. The random weight can belong to any neuron and any layer. This process of choosing a random weight, calculating the numerical gradient, comparing it to the analytic gradient is then repeated 100 times. The test asserts that the relative error is less than $10^{-7}$ each time. A portion of the computed analytic and numerical gradients are shown in figure \ref{num-grads}.
\begin{figure}
	\begin{lstlisting}
Layer: 2, Neuron: 0,  Weight: 31 
Analytic Gradient: -0.0638284 Numerical Gradient: -0.0638284

Layer: 0, Neuron: 93, Weight: 71 
Analytic Gradient: -0.156235  Numerical Gradient: -0.156235

Layer: 1, Neuron: 34, Weight: 29 
Analytic Gradient: -1.22615   Numerical Gradient: -1.22615

Layer: 1, Neuron: 12, Weight: 43 
Analytic Gradient: 0.376021   Numerical Gradient: 0.376021		
	\end{lstlisting}
	\caption{Results from the fully connected test using randomly sampled weights to perform gradient checking}
	\label{num-grads}
\end{figure}

The convolutional gradient checking test is set up in the same manner as the fully connected gradient checking test, except that the network structure is different. The network is now a \textbf{convolutional layer --- pooling layer --- convolutional layer --- fully connected layer}. The input is randomized 8x8 data, and convolutional layers use 3$\times$3 filters with a padding and stride set to 1. The first convolutional layer has 3 output channels and the second convolutional layer has 3 input channels and 6 output channels. The code used to create the network is shown in figure \ref{conv-net}.

\begin{figure}
	\begin{lstlisting}[language=C++]
int    input_size   = 8*8;
int    output_size  = 2;
int    batch_size   = 1;
double momentum     = 0.9;
double lr           = 0.001; 
Net net(input_size, output_size, batch_size, lr, momentum);

Layer* conv1 = new ConvLayer(8, 3, 1, 1, 1, 3);
Layer* pool1 = new PoolingLayer(8, 4, 3);
Layer* conv2 = new ConvLayer(4, 3, 1, 1, 3, 6);
Layer* fc1   = new FullyConnected(4*4*6, output_size);

net.addLayer(conv1);
net.addLayer(pool1);
net.addLayer(conv2);
net.addLayer(fc1);	
	\end{lstlisting}
	\caption{Layer created for the convolutional layer gradient check test.}
	\label{conv-net}
\end{figure}


\subsection{Building and Running the Test Suites}
The test suites requires Google Test to compile. Google Test can be downloaded online at GitHub \footnote{\url{https://github.com/google/googletest}}. The \textit{googletest} directory should then be placed under the \textit{SWModel} folder. The test suite can then be compiled using the provided Makefile and the following command:
\begin{lstlisting}
> make all_tests
\end{lstlisting}
This will produce an executable in the \textit{SWModel} directory called \textbf{all\_tests}. The test suites can be run by invoking the executable. The output is shown in figure \ref{goog-out}
\begin{figure}

\begin{framed}
\begin{alltt}
> ./all\_tests
Running main() from ./googletest/src/gtest\_main.cc
{\color{mygreen}[==========]} Running 6 tests from 4 test cases.
{\color{mygreen}[----------]} Global test environment set-up.
{\color{mygreen}[----------]} 1 test from ConvTest
{\color{mygreen}[ RUN      ]} ConvTest.TestForward
{\color{mygreen}[       OK ]} ConvTest.TestForward (1 ms)
{\color{mygreen}[----------]} 1 test from ConvTest (11 ms total)

{\color{mygreen}[----------]} 1 test from FCTest
{\color{mygreen}[ RUN      ]} FCTest.TestForward
{\color{mygreen}[       OK ]} FCTest.TestForward (0 ms)
{\color{mygreen}[----------]} 1 test from FCTest (10 ms total)

{\color{mygreen}[----------]} 2 tests from NeuronTest
{\color{mygreen}[ RUN      ]} NeuronTest.InitWeights
{\color{mygreen}[       OK ]} NeuronTest.InitWeights (0 ms)
{\color{mygreen}[ RUN      ]} NeuronTest.SetWeightsAndGetOutput
{\color{mygreen}[       OK ]} NeuronTest.SetWeightsAndGetOutput (0 ms)
{\color{mygreen}[----------]} 2 tests from NeuronTest (29 ms total)

{\color{mygreen}[----------]} 2 tests from GradientTest
{\color{mygreen}[ RUN      ]} GradientTest.FCGradientCheck
{\color{mygreen}[       OK ]} GradientTest.FCGradientCheck (950 ms)
{\color{mygreen}[ RUN      ]} GradientTest.ConvGradientCheck
{\color{mygreen}[       OK ]} GradientTest.ConvGradientCheck (2260 ms)
{\color{mygreen}[----------]} 2 tests from GradientTest (3223 ms total)

{\color{mygreen}[----------]} Global test environment tear-down
{\color{mygreen}[==========]} 6 tests from 4 test cases ran. (3329 ms total)
{\color{mygreen}[  PASSED  ]} 6 tests.
\end{alltt}
\end{framed}
\caption{Test coverage output using the Google Test C++ testing framework to verify the correctness of the software model for both forward and backward passes.}
\label{goog-out}
\end{figure}

