\chapter{Results}\label{results}
\section{Benchmarking Models and Structure}
Some of the results in this chapter are based on evaluating the hardware model (HWM) against other models implementing the same neural network. The other models include my software model (SWM), PyTorch running on the CPU (PyCPU), and PyTorch running on the GPU (PyGPU). My software model performs the same computations as the hardware model, so this provides insight to speedup over CPU without computational optimizations. The PyTorch CPU and GPU models then compare my hardware accelerator against heavily-optimized neural network frameworks. A training epoch in the following experiments is defined as performing learning on the 60,000 training images of the MNIST dataset. Inference experiments measure time to perform inference on all 70,000 images in the MNIST dataset.

\section{Parallelism in GPU-based Training}
Figure \ref{gpu-speedup} shows PyGPU speedup for 1 training epoch over the 60,000 training images for varying batch sizes. As one might notice, speedup grows at approximately the same rate as batch size. This is a textbook display of data parallelism; where it is clear that the images in the batch are processed individually by separate CUDA cores. The slope is able to stay linear even at massive batch sizes because a larger batch size means fewer updates to the weights. This results in the overhead from a larger batch size being negated by the decrease in amount of weight updates. The amount of forward and backward passes, which can be completely parallelized, remains the same. The weight update, the serial portion of training, is only performed once per batch. This means that a batch size of 1 performs 5 times more weight updates than a batch size of 5, which performs 20 times more weight updates than a batch size of 100. Therefore it is for this reason to see such a  speedup.  

From the figures, it is clear that the GPU model takes advantage of data-level parallelism to achieve performance, as epoch time is a near linear function of batch size. As a result, since the GPU-based implementation uses a coarser form of parallelism compared to the HWM, it would be illogical to benchmark speedup against the GPU with a batch size of 1 since the GPU's parallelism depends on batch size. Therefore, in the following experiments, the PyGPU model has been benchmarked using a batch size of 50 unless otherwise specified. It should be noted that the PyGPU model also performs 49 fewer weight updates compared to the other models as a result of this. Moreover, each weight update on a GPU would require reductions of partial gradient results from the CUDA kernels, so this should be taken into consideration when observing the following performance benchmarks.

\begin{figure}
	\centering
	\begin{tikzpicture}
	\begin{axis}
	[xmin=1, ymin=1, ylabel={Speedup}, ytick={1, 100, 200, 300, 400, 500, 600}, xtick={1, 50, 100, 200, 500}, xlabel={Batch Size}]
	\addplot coordinates 
	{(1,1) (5, 5.59) (10, 10.436) (50, 56.178) (100, 103.73) (200, 205.404) (500, 587.764)};
	\end{axis}
	\end{tikzpicture}
	\caption{Speedup for 1 training epoch when training using different batch sizes for PyGPU. Speedup is calculated using the PyGPU model with a batch size of 1 as the reference.}
	\label{gpu-speedup}
\end{figure}

\section{Evaluation Hardware}
The hardware model is evaluated using a ZedBoard equipped with a Zynq-7000 XC7Z020 SoC. The SWM and PyCPU both run on a Intel Core i7-4720HQ CPU. The GPU is an Nvidia GeForce GTX 970M equipped with 6 GB of GDDR5 RAM.

\section{Performance}
One of the most important metrics for an accelerator is runtime performance. 
While this hardware model is primarily focused on training, experiments to determine performance for both training and inference modes have both been conducted and are shown in this section.

\subsection{Training}
The average time for 1 training epoch has been recorded for each of the neural network models. The result is shown in Figure \ref{train-runtime-res}. This graph shows that the accelerator massively outperforms CPU models. Figure \ref{train-speedup-res} shows the speedup of the models, using PyCPU as a baseline. Notably, the HWM achieves a speedup of of nearly equal to that of the PyGPU model.

\begin{figure}
	\centering
	\begin{tikzpicture}
	\begin{axis}[
	ybar,
	width=3.5in,
	enlarge y limits=false,
	enlarge x limits=0.15,
	legend style={at={(0.5,-0.15)},
		anchor=north,legend columns=-1},
	ylabel={Runtime (s)},
	ymin=0,
	ymax=10,
	xmax=SWM,
	restrict y to domain*=0:12, % Cut values off at 14
	visualization depends on=rawy\as\rawy, % Save the unclipped values
	after end axis/.code={ % Draw line indicating break
		\draw [ultra thick, white, decoration={snake, amplitude=1pt}, decorate] (rel axis cs:0.1,1.0) -- (rel axis cs:1.0,1.0);},
	axis lines*=left,
	clip=false,
	nodes near coords={{\pgfmathprintnumber{\rawy}}},
	symbolic x coords={HWM, PyGPU, PyCPU, SWM},
	xtick={data}
	]
	
	\addplot coordinates {(HWM, 5.455)  (PyGPU, 4.9689) (PyCPU, 94.633) (SWM, 272.67)};
	
	\end{axis}
	\end{tikzpicture}
	\caption{Training runtime for various network models}
	\label{train-runtime-res}
\end{figure}

\begin{figure}
	\centering
	\begin{tikzpicture}
	\begin{axis}[
	ybar,
	width=3.5in,	height=2in,
	enlarge y limits=false,
	enlarge x limits=0.15,
	legend style={at={(0.5,-0.15)},
		anchor=north,legend columns=-1},
	ylabel={Speedup},
	ymin=0,
	ymax=22,
	xmin=HWM,
	xmax=SWM,
	symbolic x coords={HWM, PyGPU, PyCPU, SWM},
	xtick={data},
	scaled y ticks=false,
	nodes near coords,
	nodes near coords style={/pgf/number format/fixed},	
	nodes near coords style={/pgf/number format/precision=3},
	nodes near coords align={vertical}
	]
	
	\addplot coordinates {(HWM, 17.348)  (PyGPU, 19.045) (PyCPU, 1) (SWM, 0.347)};
	
	\end{axis}
	\end{tikzpicture}
	\caption{Training speedup using the PyCPU as a baseline}
	\label{train-speedup-res}
\end{figure}


\subsection{Inference}
Inference performance was also measured for each of the models. The result is shown in Figure \ref{inf-runtime-res}. This graph shows that the accelerator also outperforms CPU models for inference, though falls short of the GPU model. Figure \ref{inf-speedup-res} shows the speedup of the models, using PyCPU as a baseline. The HWM achieves a speedup of 2.282 compared to the PyCPU model.

\begin{figure}
	\centering
	\begin{tikzpicture}
	\begin{axis}[
	ybar,
	width=3.5in,
	enlarge y limits=false,
	enlarge x limits=0.15,
	legend style={at={(0.5,-0.15)},
		anchor=north,legend columns=-1},
	ylabel={Runtime (s)},
	ymin=0,
	ymax=10,
	xmax=SWM,
	restrict y to domain*=0:12, % Cut values off at 14
	visualization depends on=rawy\as\rawy, % Save the unclipped values
	after end axis/.code={ % Draw line indicating break
		\draw [ultra thick, white, decoration={snake, amplitude=1pt}, decorate] (rel axis cs:0.1,1.0) -- (rel axis cs:1.0,1.0);},
	axis lines*=left,
	clip=false,
	symbolic x coords={HWM, PyGPU, PyCPU, SWM}, 
	nodes near coords={{\pgfmathprintnumber{\rawy}}},
	xtick={data}
	]
	
	\addplot 
	coordinates {(HWM, 5.392)  (PyGPU, 1.32) (PyCPU, 12.305) (SWM, 56.808)};
	
	\end{axis}
	\end{tikzpicture}
	\caption{Inference runtime for various network models}
	\label{inf-runtime-res}
\end{figure}

\begin{figure}
	\centering
	\begin{tikzpicture}
	\begin{axis}[
	ybar,
	height=2in,
	width=3.5in,
	enlarge y limits=false,
	enlarge x limits=0.15,
	legend style={at={(0.5,-0.15)},
		anchor=north,legend columns=-1},
	ylabel={Speedup},
	ymin=0,
	ymax=11,
	xmin=HWM,
	xmax=SWM,
	symbolic x coords={HWM, PyGPU, PyCPU, SWM},
	xtick={data},
	scaled y ticks=false,
	nodes near coords,
	nodes near coords style={/pgf/number format/fixed},	
	nodes near coords style={/pgf/number format/precision=3},
	nodes near coords align={vertical}
	]
	
	\addplot coordinates {(HWM, 2.282)  (PyGPU, 9.322) (PyCPU, 1) (SWM, 0.217)};
	
	\end{axis}
	\end{tikzpicture}
	\caption{Inference speedup using the PyCPU as a baseline}
	\label{inf-speedup-res}
\end{figure}

\subsection{Active/Idle Cycles}
To determine the impact of using MMIO via AXI bus to transfer image data between the PS and the FPGA, active and idle cycles were measured during training and inference. An active cycle is defined as a cycle on the FPGA during which at least one of the layers was computationally active. An idle cycle is thus defined as a non-active cycle. 

An experiment was performed to measure active cycle percentages for the HWM during inference and training. The dataset was the entire MNIST dataset in both cases. The active cycle percentage for inference and training are shown in Table \ref{active-cycle-table}.

\begin{table}
	\centering 
	\begin{tabular}{|c|c|}
		\hline
		& \textbf{Active Cycle Percentage} \\\hline
		Inference & 25.13\% \\\hline 
		Training & 69.20\% \\\hline
	\end{tabular}
	\caption{Active Cycle Percentages for inference and training.}
	\label{active-cycle-table}
\end{table}

This experiment was performed to evaluate if the sending of input over MMIO was the bottleneck of the system. As can be clearly seen from the table, the MMIO transfer of training data was indeed the bottleneck. Furthermore, since backpropagation requires roughly double the amount of work compared to inference, it makes sense that training (which is both inference and backpropagation) is roughly a factor of 3 more active.

\section{Training Accuracy}
This section details the accuracy of the training process using the hardware accelerator. Varying training dataset sizes were chosen during the training process as the reduced precision training resulted in non-convergent training. As such, the training accuracy experiment conducted modified two variables: the learning rate and the training dataset size. 

The tested learning rates were $2^{-7}$, $2^{-8}$, $2^{-9}$, and $2^{-10}$ (0.0078, 0.0039, 0.00195, and 0.000977). This is because the hardware model performs the learning rate multiplication by using bitshifts. The experiments recorded the peak test data set accuracy during the training process. Note that the test dataset size for each run is 70000 minus the size of the training dataset. The results are shown in Figure \ref{training-accuracy}. In this experiment, the highest test set accuracy, 85.845\%, was achieved with a learning rate of $\eta = 2^{-9}$ and with a dataset of size 4,000. When using the same neural network architecture as the HWM, the SWM converges to 97.6\% test set accuracy.

\begin{figure}
	\centering
	\begin{tikzpicture}
	\begin{axis}[
	ybar,
	height=3in,
	width=\textwidth,
	enlarge y limits=false,
	enlarge x limits=0.15,
	legend style={at={(0.5,-0.15)},
		anchor=north,legend columns=-1},
	ylabel={Peak Accuracy (\%)},
	xlabel={Training Dataset Size},
	ymin=50,
	ymax=90,
	xtick={data},
	scaled y ticks=false,	
	scaled x ticks=false,
	]
	
	\addplot coordinates {(2000, 74.401)  (4000, 72.814) (6000, 74.49) (8000, 61.03) (10000, 57.34)};
	
	\addplot coordinates {(2000, 81.14)  (4000, 78.94) (6000, 75.70) (8000, 76.09) (10000, 78.36)};
	
	\addplot coordinates {(2000, 81.97)  (4000, 85.845) (6000, 82.737) (8000, 84.02) (10000, 79.62)};
	
	\addplot coordinates {(2000, 83.79)  (4000, 78.8) (6000, 83.78) (8000, 81.11) (10000, 81.98)};
	\legend{$\eta = 2^{-7}$, $\eta = 2^{-8}$, $\eta = 2^{-9}$, $\eta = 2^{-10}$}
	\end{axis}
	\end{tikzpicture}
	\caption{Maximum training accuracy reached for various learning rate and training set sizes.}
	\label{training-accuracy}
\end{figure}

\subsection{Stability of Training}
As previously mentioned, due to the relatively low training precision of 18-bit fixed-point, the training process does not converge to a maximum training accuracy, but rather it will reach a maximum training accuracy, and then accuracy will degrade as precision errors accumulate over the training process. Training statistics for the first 10 epochs of the most optimal training configuration from Figure \ref{training-accuracy} illustrate this phenomenon and are shown in Figure \ref{epoch-by-epoch}.

\begin{figure}
	\centering
	\begin{tikzpicture}
	\begin{axis}[
	ylabel={Accuracy (\%)}, 
	xlabel={Epoch},
	width=\textwidth,
	height=2.5in,
	]
	
	\addplot coordinates 
	{(1, 48.12) (2, 75.012) (3, 83.62) (4, 85.85) (5, 69.99) (6, 70.18) (7, 55.43) (8, 55.78) (9, 52.48) (10, 51.53)};
	\addplot coordinates 	
	{(1, 61.85) (2, 73.45) (3, 84.55) (4, 85.55) (5, 83.78) (6, 75.42) (7, 67.03) (8, 58.48) (9, 54.6) (10, 50.8)};
	
	\legend{Test, Train}
	\end{axis}
	\end{tikzpicture}
	\caption{Epoch-by-epoch training data for an HWM configuration. Clearly visible degradation of accuracy instead of convergence after epoch 4.}
	\label{epoch-by-epoch}
\end{figure}


\section{Implemented Design}
The design implemented for the FPGA is shown in Figure \ref{impl-design}. As expected, the FC1 layer is by and large the most resource intensive, as it utilizes 196 kernels. It is interesting to observe the clustering of individual layer modules, while the interlayer activation buffer for FC0 and FC1 is widely spread out through the FPGA. This would indicate that this interlayer activation buffer was frequently routed to as a midpoint between FC0 and FC1. 

It should be noted that implementation is a non-deterministic process and every design run should result in a slightly different implemented design. However, general trends for routing of the design tend to  persist throughout multiple runs, despite the non-determism of the placing and routing algorithms.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{figures/impl_design}
	\includegraphics{figures/impl_design_legend}
	\caption{The implemented design of the hardware model}
	\label{impl-design}
\end{figure}

\subsection{Resource Usage, Power, and Timing}
The resource usage of the hardware model is shown in Table \ref{resource-usage}. As can be seen, the DSP slice is the scarcest resource, with LUTs and BRAMs also heavily being used.  Overall, high utilization of the FPGA resources were made to optimize the performance of the accelerator as much as possible.
\begin{table}
	\centering 
	\begin{tabular}{|l|l|l|c|}
	\hline 
	\textbf{Resource} & \textbf{Utilization} & \textbf{Available} & \textbf{Utilization \%} \\\hline 
	LUT & 41132 & 53200 & 77.32		\\\hline
	FF & 54097 & 106400 & 50.84 \\\hline 
	BRAM & 107.5 & 140 & 76.79 \\\hline 
	DSP & 215 & 220 & 97.73 \\\hline	
	\end{tabular}
\caption{Resource usage of the implemented design}
\label{resource-usage}
\end{table}

According to the Vivado report, the total on-chip power of the design is 2.798 Watts. While this number is reported with `Low' confidence by Vivado, this wattage is far lower than typical GPU power consumptions. Power measurements have not been made for the internal GPU during these experiments, though measurements using the Torch framework were made and reported GPU average power at 94.19 Watts while performing training using the AlexNet architecture.The power measurements were performed with an Nvidia Tesla K20M GPU with 5 GB of GDDR5 SDRAM, a top-of-the-line GPU \cite{xinbochen2016}. This is several magnitudes higher than the FPGA based solution in this thesis. 

The implemented design for the hardware model is clocked at a frequency of 50 MHz. Placement and routing are both able to successfully complete with 0 timing violations. There was no need to improve frequency because the current performance bottleneck of this design stems from data transfer over the AXI bus and not from FPGA computational speed.