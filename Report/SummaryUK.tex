\chapter{Abstract}

This thesis proposes a novel hardware architecture to accelerate the training of neural networks with small batch sizes. The accelerator uses a modular, parameterizable and computationally well-balanced design to successfully implement high-performance online training of neural networks. By using fine-grained parallelism at the neuron level, the accelerator was able to achieve a speedup of 17.3 against a PyTorch CPU implementation for a specific neural network architecture. The accelerator also performs nearly as fast as a PyTorch GPU implementation of the network that used a batch size of 50 during training.

This thesis also highlights the importance of high-precision calculation for training. The highest accuracy attained by the accelerator on the MNIST dataset was 85.845\%, which is a result of 18-bit fixed-point precision being unable to successfully converge to a local optima as a result of the accumulation of precision error causing the degradation of training accuracy after a few epochs.

